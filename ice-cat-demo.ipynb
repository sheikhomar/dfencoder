{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17ab9f4-0f9d-418f-b675-9935d42026e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dfencoder\n",
    "from dfencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c57ef567-1363-47a2-a832-d87ce143f341",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_value_lengths(dataframe: pd.DataFrame, col_name: str):\n",
    "    unique_vals = map(lambda r: str(r), dataframe[col_name].unique())\n",
    "    unique_vals = filter(lambda r: r != 'nan', unique_vals)\n",
    "    unique_vals = map(lambda r: len(r), unique_vals)\n",
    "    unique_vals = list(unique_vals)\n",
    "    return np.array(unique_vals)\n",
    "\n",
    "def inferred_type(dataframe: pd.DataFrame, col_name: str, max_cat_value_count: int=1000) -> np.dtype:\n",
    "    is_datetime_col = dataframe[col_name].str.match('(\\d{2,4}(-|\\/|\\\\|\\.| )\\d{2}(-|\\/|\\\\|\\.| )\\d{2,4})+').all()\n",
    "    if is_datetime_col:\n",
    "        return 'datetime'\n",
    "    \n",
    "    is_int32 = dataframe[col_name].str.match('\\d{1,8}$').all()\n",
    "    if is_int32:\n",
    "        return 'int32'\n",
    "    \n",
    "    is_float = dataframe[col_name].str.match(r'\\d{1,7}(\\.\\d{1,5})?$').all()\n",
    "    if is_float:\n",
    "        return 'float'\n",
    "    \n",
    "    unique_vals = dataframe[col_name].unique()\n",
    "    n_unique = unique_vals.shape[0]\n",
    "\n",
    "    if n_unique == 2 or n_unique == 3:\n",
    "        bool_vals = np.array(['(N/A)', 'N', 'Y'], dtype='str')\n",
    "        possible_bool_vals = np.array(pd.DataFrame(unique_vals).fillna('(N/A)')[0])\n",
    "        if np.isin(possible_bool_vals, bool_vals).all():\n",
    "            return 'bool'\n",
    "    \n",
    "    if n_unique >= 2 and n_unique < max_cat_value_count:\n",
    "        unique_val_lengths = get_unique_value_lengths(dataframe, col_name)\n",
    "        if np.max(unique_val_lengths) > 300:\n",
    "            # print(f'{col_name} -> {np.max(unique_val_lengths)}')\n",
    "            return 'object'\n",
    "        return 'category'\n",
    "\n",
    "    return 'object'\n",
    "\n",
    "def clean_icecat(dataframe: pd.DataFrame):\n",
    "    # Count number of rows per category\n",
    "    df_count_by_category = dataframe.groupby('category_name').agg({'id': 'count'}).rename(columns={'id': 'n_rows'})\n",
    "\n",
    "    # Find categories with at least N amount of rows\n",
    "    categories = list(df_count_by_category[df_count_by_category.n_rows > 20].index)\n",
    "\n",
    "    # Delete rows with few that N amount of rows per category\n",
    "    dataframe = dataframe[dataframe.category_name.isin(categories)]\n",
    "\n",
    "    # Get columns that specify features of the products\n",
    "    product_feature_columns = list(dataframe.columns)[26:]\n",
    "\n",
    "    # Find columns that have too few specified values\n",
    "    n_rows = dataframe.shape[0]\n",
    "    small_columns = []\n",
    "    for col in product_feature_columns:\n",
    "        n_filled = n_rows - dataframe[col].isna().sum()\n",
    "        if n_filled < 10:\n",
    "            small_columns.append(col)\n",
    "\n",
    "    # Find columns that have enough values\n",
    "    product_features_to_use = [col for col in product_feature_columns if col not in small_columns]\n",
    "    \n",
    "    # Create a copy\n",
    "    df_cleaned = dataframe[['category_name', 'supplier_name'] + product_features_to_use].copy()\n",
    "    \n",
    "    # Use proper dtypes\n",
    "    for col in df_cleaned.columns:\n",
    "        dtype = inferred_type(df_cleaned, col)\n",
    "        if dtype == 'int32':\n",
    "            df_cleaned[col].fillna(0, inplace=True)\n",
    "        elif dtype == 'float':\n",
    "            df_cleaned[col].fillna(0.0, inplace=True)\n",
    "        elif dtype == 'bool':\n",
    "            df_cleaned[col].fillna('N', inplace=True)\n",
    "            df_cleaned[col] = df_cleaned[col].str.replace('N', '0')\n",
    "            df_cleaned[col] = df_cleaned[col].str.replace('Y', '1')\n",
    "            df_cleaned[col] = df_cleaned[col].astype('int')\n",
    "        elif dtype == 'category':\n",
    "            df_cleaned[col].fillna('(N/A)', inplace=True)\n",
    "            \n",
    "        df_cleaned[col] = df_cleaned[col].astype(dtype)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "def split_train_test(df):\n",
    "    train = df.sample(frac=.8, random_state=42)\n",
    "    test = df.loc[~df.index.isin(train.index)]\n",
    "    return train, test\n",
    "\n",
    "def compute_column_stats(dataframe: pd.DataFrame):\n",
    "    dmap = {\n",
    "        'column': [],\n",
    "        'suggested_type': [],\n",
    "        'n_unique': [],\n",
    "        'len_total': [],\n",
    "        'len_min': [],\n",
    "        'len_max': [],\n",
    "        'len_avg': [],\n",
    "        'values': [],\n",
    "        'n_filled': [],\n",
    "    }\n",
    "\n",
    "    n_rows = dataframe.shape[0]\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        dmap['column'].append(col)\n",
    "        dmap['suggested_type'].append(inferred_type(dataframe, col))\n",
    "\n",
    "        dmap['n_filled'].append( n_rows - dataframe[col].isna().sum() )\n",
    "\n",
    "        dmap['n_unique'].append(dataframe[col].unique().shape[0])\n",
    "        unique_val_lengths = get_unique_value_lengths(dataframe, col)\n",
    "        dmap['len_total'].append(len(unique_val_lengths))\n",
    "        dmap['len_min'].append(np.min(unique_val_lengths))\n",
    "        dmap['len_max'].append(np.max(unique_val_lengths))\n",
    "        dmap['len_avg'].append(np.mean(unique_val_lengths))\n",
    "\n",
    "        vals = ' | '.join([str(s) for s in list(dataframe[col].unique())[0:5]])\n",
    "        dmap['values'].append(vals)\n",
    "\n",
    "    return pd.DataFrame(dmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ecd314-8685-4811-94f0-145fb792ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('ice-cat-office-products.csv.gz', dtype=str, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac69e9f6-5e36-49d2-b7dd-0c3ff927d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_data = clean_icecat(df_data)\n",
    "df_train, df_test = split_train_test(df_cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6554fda-c772-46a1-89ac-6fded846db78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09f2429-149f-4ad9-a70f-ad451ef95341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dfencoder.AutoEncoder(\n",
    "    encoder_layers = [512, 512, 512], #model architecture\n",
    "    decoder_layers = [], #decoder optional - you can create bottlenecks if you like\n",
    "    activation='relu',\n",
    "    swap_p=0.2, #noise parameter\n",
    "    lr = 0.01,\n",
    "    lr_decay=.99,\n",
    "    batch_size=512,\n",
    "    logger_name='ipynb', #special logging for jupyter notebooks\n",
    "    verbose=False,\n",
    "    optimizer='sgd',\n",
    "    scaler='gauss_rank', #gauss rank scaling forces your numeric features into standard normal distributions\n",
    "    min_cats=3 #Define cutoff for minority categories, default 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b23d6-2c5a-4ffd-978b-da426608897e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de743979-1731-4763-8079-5b1eced18fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec8be6d5e795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/dfencoder/dfencoder/autoencoder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, df, epochs, val)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0minput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_decay\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/dfencoder/dfencoder/autoencoder.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, n_updates, input_df, df, pbar)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             )\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/dfencoder/dfencoder/autoencoder.py\u001b[0m in \u001b[0;36mdo_backward\u001b[0;34m(self, mse, bce, cce)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_baseline_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dfencoder-2nDEsGtJ-py3.8/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/dfencoder-2nDEsGtJ-py3.8/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(df_train, epochs=10, val=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54670fc-8980-4095-90de-d68582496fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
